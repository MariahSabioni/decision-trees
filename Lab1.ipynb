{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9798cd52",
   "metadata": {},
   "source": [
    "## DD2421 Machine Learning - Lab 1: Decision Trees  \n",
    "Haocheng Zhang & Mariah Sabioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a0930",
   "metadata": {},
   "source": [
    "### Assignment 0\n",
    "Each one of the datasets has properties which makes\n",
    "them hard to learn. Motivate which of the three problems is most\n",
    "difficult for a decision tree algorithm to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35688e24",
   "metadata": {},
   "source": [
    "The most difficult problem would be problem 2, because on this problem there is no \"best question\" to start the tree and you cannot terminate the tree and \"throw away\" any of the features to define the class label, therefore the tree has an exponential number of nodes.  \n",
    "The algorithm has to ask 6 questions, one for each feature, because knowing one answer does not change the probability of the next (there are no redundant features).  \n",
    "It is also interesting to notice that it is the only of the problems that was not expressed in logic form. It would tke quite a number of lines to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c20b4",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "The file dtree.py defines a function entropy which\n",
    "calculates the entropy of a dataset. Import this file along with the\n",
    "monks datasets and use it to calculate the entropy of the training\n",
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37cb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtree import entropy, averageGain, select, bestAttribute, buildTree, check, mostCommon, allPruned\n",
    "from drawtree_qt5 import drawTree\n",
    "import monkdata as m\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd4a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['MONK-1', 'MONK-2', 'MONK-3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1d4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent1 = entropy(m.monk1)\n",
    "ent2 = entropy(m.monk2)\n",
    "ent3 = entropy(m.monk3)\n",
    "ents = [ent1, ent2, ent3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079d6a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Dataset   |   Entropy |\n",
      "|-----------|-----------|\n",
      "| MONK-1    |  1        |\n",
      "| MONK-2    |  0.957117 |\n",
      "| MONK-3    |  0.999806 |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate({\"Dataset\": dataset_names,\"Entropy\": ents}, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eedfcd",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "Explain entropy for a uniform distribution and a\n",
    "non-uniform distribution, present some example distributions with\n",
    "high and low entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf150be",
   "metadata": {},
   "source": [
    "Entropy in simple terms is a measurement of chaos. Therefore the higher the unpredictability, the higher the entropy.\n",
    "* Uniform distributions:  \n",
    "Dice (or the honest ones) have uniform distributions. However, if you compare a 6-side die and a 20-side die, the entropy is bigger for the 20-side die. That is because it is harder to know the outcome for the 20-side than it is for the 6-side (if you guess, you have less chance to be right on the 20-side).\n",
    "* Non-uniform distributions:  \n",
    "A fake coin is an example of a non-uniform distribution. In that case, the \"faker\" (i.e., the larger the probability of one of the sides), the lower the entropy. That makes sense, since it becomes easier to know the outcome, therefore the uncertainty is lower.\n",
    "* Uniform vs. non-uniform distributions:  \n",
    "If we compare the honest and the fake coin (uniform and non-uniform, respectively), the entropy will be lower for the fake coin. That is because the bias reduces the uncertainty. In fact, the uniform distribution contains the higher uncertainty (= higher entropy). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248a040",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "Use the function averageGain (defined in dtree.py)\n",
    "to calculate the expected information gain corresponding to each of\n",
    "the six attributes. Note that the attributes are represented as instances of the class Attribute (defined in monkdata.py) which you can access via m.attributes[0], ..., m.attributes[5]. Based on\n",
    "the results, which attribute should be used for splitting the examples\n",
    "at the root node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5272fc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| dataset   |         A1 |         A2 |          A3 |         A4 |        A5 |          A6 |\n",
      "|-----------|------------|------------|-------------|------------|-----------|-------------|\n",
      "| MONK-1    | 0.0752726  | 0.00583843 | 0.00470757  | 0.0263117  | 0.287031  | 0.000757856 |\n",
      "| MONK-2    | 0.00375618 | 0.0024585  | 0.00105615  | 0.0156642  | 0.0172772 | 0.00624762  |\n",
      "| MONK-3    | 0.00712087 | 0.293736   | 0.000831114 | 0.00289182 | 0.255912  | 0.00707703  |\n"
     ]
    }
   ],
   "source": [
    "datasets = [m.monk1, m.monk2, m.monk3]\n",
    "gains = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    gain = {}\n",
    "    gain['dataset'] = dataset_names[i]\n",
    "    for attribute in m.attributes:\n",
    "        gain[attribute] = averageGain(dataset, attribute)\n",
    "    gains.append(gain)\n",
    "print(tabulate(gains, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64afbc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root node should provide the largest information gain calculated above:\n",
      "MONK-1: root node A5 | information gain = 0.28703074971578435\n",
      "MONK-2: root node A5 | information gain = 0.01727717693791797\n",
      "MONK-3: root node A2 | information gain = 0.29373617350838865\n"
     ]
    }
   ],
   "source": [
    "print('The root node should provide the largest information gain calculated above:')\n",
    "for i, _ in enumerate(gains):\n",
    "    gain_values = []\n",
    "    gain_attributes = []\n",
    "    for attribute in m.attributes:\n",
    "        gain_values.append(gains[i][attribute])\n",
    "        gain_attributes.append(attribute)\n",
    "    print(f'{dataset_names[i]}: root node {gain_attributes[gain_values.index(max(gain_values))]} | '\n",
    "          + f'information gain = {max(gain_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9806d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Or, using the defined bestAttribute function:\n",
      "MONK-1: root node A5\n",
      "MONK-2: root node A5\n",
      "MONK-3: root node A2\n"
     ]
    }
   ],
   "source": [
    "best_attribute = []\n",
    "print('Or, using the defined bestAttribute function:')\n",
    "for i, dataset in enumerate(datasets):\n",
    "    best_attribute = bestAttribute(dataset, m.attributes)\n",
    "    print(f'{dataset_names[i]}: root node {best_attribute}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c492e",
   "metadata": {},
   "source": [
    "### Assignment 4\n",
    "For splitting we choose the attribute that maximizes\n",
    "the information gain, Eq.3. Looking at Eq.3 how does the entropy of\n",
    "the subsets, Sk, look like when the information gain is maximized?\n",
    "How can we motivate using the information gain as a heuristic for\n",
    "picking an attribute for splitting? Think about reduction in entropy\n",
    "after the split and what the entropy implies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358996f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1\n",
      "> attribute: A5\n",
      "> entropy before split: S = 1.0\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9990797181805819\n",
      ">> S1(A5=2) = 0.9993759069576514\n",
      ">> S2(A5=3) = 0.0\n",
      ">> S3(A5=4) = 0.0\n",
      "> information gain after split: 0.28703074971578435\n",
      "MONK-2\n",
      "> attribute: A5\n",
      "> entropy before split: S = 0.957117428264771\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9182958340544896\n",
      ">> S1(A5=2) = 0.9830605548016025\n",
      ">> S2(A5=3) = 0.0\n",
      ">> S3(A5=4) = 0.0\n",
      "> information gain after split: 0.01727717693791797\n",
      "MONK-3\n",
      "> attribute: A2\n",
      "> entropy before split: S = 0.9998061328047111\n",
      "> entropy after split:\n",
      ">> S0(A2=1) = 0.9898220559635811\n",
      ">> S1(A2=2) = 0.9954515828457715\n",
      ">> S2(A2=3) = 0.0\n",
      "> information gain after split: 0.29373617350838865\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(datasets):\n",
    "    best_attribute = bestAttribute(dataset, m.attributes)\n",
    "    print(f'{dataset_names[i]}')\n",
    "    print(f'> attribute: {best_attribute}')\n",
    "    print(f'> entropy before split: S = {ents[i]}')\n",
    "    print(f'> entropy after split:') \n",
    "    for k, v in enumerate(best_attribute.values):\n",
    "        subset = select(dataset, attribute, v)\n",
    "        subset_entropy = entropy(subset)\n",
    "        print(f'>> S{k}({best_attribute}={v}) = {subset_entropy}')\n",
    "    print(f'> information gain after split: {averageGain(dataset, best_attribute)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d71906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________\n",
      "MONK-1\n",
      "> entropy before split: S = 1.0\n",
      "------------------------------------------------------------------\n",
      "> attribute: A1\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.8944518845341284\n",
      ">> S1(A5=2) = 0.9983636725938131\n",
      ">> S2(A5=3) = 0.8779620013943912\n",
      "> information gain after split: 0.07527255560831925\n",
      "------------------------------------------------------------------\n",
      "> attribute: A2\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9852281360342516\n",
      ">> S1(A5=2) = 0.9983636725938131\n",
      ">> S2(A5=3) = 0.9970590569034107\n",
      "> information gain after split: 0.005838429962909286\n",
      "------------------------------------------------------------------\n",
      "> attribute: A3\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9957274520849256\n",
      ">> S1(A5=2) = 0.9948131754904235\n",
      "> information gain after split: 0.00470756661729721\n",
      "------------------------------------------------------------------\n",
      "> attribute: A4\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9587118829771318\n",
      ">> S1(A5=2) = 0.961236604722876\n",
      ">> S2(A5=3) = 0.9996098363678071\n",
      "> information gain after split: 0.02631169650768228\n",
      "------------------------------------------------------------------\n",
      "> BEST ATTRIBUTE: A5\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.0\n",
      ">> S1(A5=2) = 0.9383153522334069\n",
      ">> S2(A5=3) = 0.9480782435939054\n",
      ">> S3(A5=4) = 0.9081783472997051\n",
      "> information gain after split: 0.28703074971578435\n",
      "------------------------------------------------------------------\n",
      "> attribute: A6\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9990797181805819\n",
      ">> S1(A5=2) = 0.9993759069576514\n",
      "> information gain after split: 0.0007578557158638421\n",
      "------------------------------------------------------------------\n",
      "__________________________________________________________________\n",
      "MONK-2\n",
      "> entropy before split: S = 0.957117428264771\n",
      "------------------------------------------------------------------\n",
      "> attribute: A1\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9182958340544896\n",
      ">> S1(A5=2) = 0.9621461334087001\n",
      ">> S2(A5=3) = 0.9805974409917271\n",
      "> information gain after split: 0.0037561773775118823\n",
      "------------------------------------------------------------------\n",
      "> attribute: A2\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9299429352091803\n",
      ">> S1(A5=2) = 0.977895682231035\n",
      ">> S2(A5=3) = 0.9526092095121206\n",
      "> information gain after split: 0.0024584986660830532\n",
      "------------------------------------------------------------------\n",
      "> attribute: A3\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9695235828220428\n",
      ">> S1(A5=2) = 0.9430685934712908\n",
      "> information gain after split: 0.0010561477158920196\n",
      "------------------------------------------------------------------\n",
      "> attribute: A4\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.8524051786494786\n",
      ">> S1(A5=2) = 0.975119064940866\n",
      ">> S2(A5=3) = 0.9904799742690307\n",
      "> information gain after split: 0.015664247292643818\n",
      "------------------------------------------------------------------\n",
      "> BEST ATTRIBUTE: A5\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9103480624345153\n",
      ">> S1(A5=2) = 1.0\n",
      ">> S2(A5=3) = 0.9633355456726842\n",
      ">> S3(A5=4) = 0.8779620013943912\n",
      "> information gain after split: 0.01727717693791797\n",
      "------------------------------------------------------------------\n",
      "> attribute: A6\n",
      "> entropy after split:\n",
      ">> S0(A5=1) = 0.9182958340544896\n",
      ">> S1(A5=2) = 0.9830605548016025\n",
      "> information gain after split: 0.006247622236881467\n",
      "------------------------------------------------------------------\n",
      "__________________________________________________________________\n",
      "MONK-3\n",
      "> entropy before split: S = 0.9998061328047111\n",
      "------------------------------------------------------------------\n",
      "> attribute: A1\n",
      "> entropy after split:\n",
      ">> S0(A2=1) = 0.9949848281859701\n",
      ">> S1(A2=2) = 0.9837082626231857\n",
      ">> S2(A2=3) = 1.0\n",
      "> information gain after split: 0.007120868396071844\n",
      "------------------------------------------------------------------\n",
      "> BEST ATTRIBUTE: A2\n",
      "> entropy after split:\n",
      ">> S0(A2=1) = 0.9182958340544896\n",
      ">> S1(A2=2) = 0.8296071030882032\n",
      ">> S2(A2=3) = 0.37764632137370036\n",
      "> information gain after split: 0.29373617350838865\n",
      "------------------------------------------------------------------\n",
      "> attribute: A3\n",
      "> entropy after split:\n",
      ">> S0(A2=1) = 0.9998292601233936\n",
      ">> S1(A2=2) = 0.9980008838722996\n",
      "> information gain after split: 0.0008311140445336207\n",
      "------------------------------------------------------------------\n",
      "> attribute: A4\n",
      "> entropy after split:\n",
      ">> S0(A2=1) = 0.99819587904281\n",
      ">> S1(A2=2) = 0.9919924034538556\n",
      ">> S2(A2=3) = 1.0\n",
      "> information gain after split: 0.002891817288654397\n",
      "------------------------------------------------------------------\n",
      "> attribute: A5\n",
      "> entropy after split:\n",
      ">> S0(A2=1) = 0.8960382325345575\n",
      ">> S1(A2=2) = 0.907165767573082\n",
      ">> S2(A2=3) = 0.9852281360342516\n",
      ">> S3(A2=4) = 0.20559250818508304\n",
      "> information gain after split: 0.25591172461972755\n",
      "------------------------------------------------------------------\n",
      "> attribute: A6\n",
      "> entropy after split:\n",
      ">> S0(A2=1) = 0.9898220559635811\n",
      ">> S1(A2=2) = 0.9954515828457715\n",
      "> information gain after split: 0.007077026074097326\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(datasets):\n",
    "    print('__________________________________________________________________')\n",
    "    print(f'{dataset_names[i]}')\n",
    "    best_attribute = bestAttribute(dataset, m.attributes)\n",
    "    print(f'> entropy before split: S = {ents[i]}')\n",
    "    print('------------------------------------------------------------------')\n",
    "    for attribute in m.attributes:\n",
    "        if(attribute == best_attribute):\n",
    "            print(f'> BEST ATTRIBUTE: {attribute}')\n",
    "        else:\n",
    "            print(f'> attribute: {attribute}')\n",
    "        print(f'> entropy after split:') \n",
    "        for k, v in enumerate(attribute.values):\n",
    "            subset = select(dataset, attribute, v)\n",
    "            subset_entropy = entropy(subset)\n",
    "            print(f'>> S{k}({best_attribute}={v}) = {subset_entropy}')\n",
    "        print(f'> information gain after split: {averageGain(dataset, attribute)}') \n",
    "        print('------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f68ac3",
   "metadata": {},
   "source": [
    "The information gain is maximized when the entropy of the subsets Sk relative to the entropy of the original dataset S is reduced by the largest amount.  \n",
    "Entropy is a measurement of uncertainty. Therefore, the attibute with larger information gain represents the attribute that, when known, provides subsets with less uncertainty (=less entropy).\n",
    "It means that when we use that variable to split the dataset, we know more about the subsets (less random = easier to predict). This allows the construction of a simpler model with better predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4a1d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1\n",
      "| subset          |        A1 |         A2 |        A3 |        A4 |         A6 |\n",
      "|-----------------|-----------|------------|-----------|-----------|------------|\n",
      "| MONK-1_subset-0 | 0         | 0          | 0         | 0         | 0          |\n",
      "| MONK-1_subset-1 | 0.0402168 | 0.0150635  | 0.0372726 | 0.0488922 | 0.0258073  |\n",
      "| MONK-1_subset-2 | 0.0330551 | 0.00219718 | 0.0179823 | 0.0191228 | 0.0451085  |\n",
      "| MONK-1_subset-3 | 0.206291  | 0.0338984  | 0.0259061 | 0.0759329 | 0.00332396 |\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets[0]\n",
    "best_attribute = bestAttribute(dataset, m.attributes)\n",
    "remaining_attributes = [a for a in m.attributes if a != best_attribute]\n",
    "subsets = []\n",
    "subset_names = []\n",
    "gains_sub = []\n",
    "for k, v in enumerate(best_attribute.values):\n",
    "    subset = select(dataset, best_attribute, v)\n",
    "    subsets.append(subset)\n",
    "    subset_names.append(dataset_names[0] + \"_subset-\" + str(k))\n",
    "print(f'{dataset_names[0]}')\n",
    "for i, subset in enumerate(subsets):\n",
    "    gain_sub = {}\n",
    "    gain_sub['subset'] = subset_names[i]\n",
    "    for attribute in remaining_attributes:\n",
    "        gain_sub[attribute] = averageGain(subset, attribute)\n",
    "    gains_sub.append(gain_sub)\n",
    "print(tabulate(gains_sub, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a220678c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom built tree (using bestAttribute and mostCommon functions:\n",
      "__________________________________________________________________\n",
      "MONK-1\n",
      "> Node 1: A5\n",
      "> information gain after split: 0.28703074971578435\n",
      "> majority class: False\n",
      "------------------------------------------------------------------\n",
      "> MONK-1_subset-0\n",
      ">> Node 2: A1\n",
      ">> information gain after split: 0.0\n",
      ">> majority class: True\n",
      "..................................................................\n",
      ">> MONK-1_subset-0_subset-0\n",
      ">>> majority class: True\n",
      "..................................................................\n",
      ">> MONK-1_subset-0_subset-1\n",
      ">>> majority class: True\n",
      "..................................................................\n",
      ">> MONK-1_subset-0_subset-2\n",
      ">>> majority class: True\n",
      "------------------------------------------------------------------\n",
      "> MONK-1_subset-1\n",
      ">> Node 2: A4\n",
      ">> information gain after split: 0.04889220262952931\n",
      ">> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-1_subset-0\n",
      ">>> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-1_subset-1\n",
      ">>> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-1_subset-2\n",
      ">>> majority class: False\n",
      "------------------------------------------------------------------\n",
      "> MONK-1_subset-2\n",
      ">> Node 2: A6\n",
      ">> information gain after split: 0.04510853782483648\n",
      ">> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-2_subset-0\n",
      ">>> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-2_subset-1\n",
      ">>> majority class: False\n",
      "------------------------------------------------------------------\n",
      "> MONK-1_subset-3\n",
      ">> Node 2: A1\n",
      ">> information gain after split: 0.20629074641530198\n",
      ">> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-3_subset-0\n",
      ">>> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-3_subset-1\n",
      ">>> majority class: False\n",
      "..................................................................\n",
      ">> MONK-1_subset-3_subset-2\n",
      ">>> majority class: True\n"
     ]
    }
   ],
   "source": [
    "print('Custom built tree (using bestAttribute and mostCommon functions:')\n",
    "print('__________________________________________________________________')\n",
    "print(f'{dataset_names[0]}')\n",
    "print(f'> Node 1: {best_attribute}')\n",
    "print(f'> information gain after split: {averageGain(dataset, best_attribute)}') \n",
    "print(f'> majority class: {mostCommon(dataset)}') \n",
    "for i, subset in enumerate(subsets):\n",
    "    best_attribute = bestAttribute(subset, remaining_attributes)\n",
    "    print('------------------------------------------------------------------')\n",
    "    print(f'> {subset_names[i]}')\n",
    "    print(f'>> Node 2: {best_attribute}')\n",
    "    print(f'>> information gain after split: {averageGain(subset, best_attribute)}')\n",
    "    print(f'>> majority class: {mostCommon(subset)}') \n",
    "    sub_subsets = []\n",
    "    sub_subset_names = []\n",
    "    for k, v in enumerate(best_attribute.values):\n",
    "        sub_subset = select(subset, best_attribute, v)\n",
    "        sub_subsets.append(sub_subset)\n",
    "        sub_subset_names.append(subset_names[i] + \"_subset-\" + str(k))\n",
    "    for j, sub_subset in enumerate(sub_subsets):\n",
    "        print('..................................................................')\n",
    "        print(f'>> {sub_subset_names[j]}')\n",
    "        print(f'>>> majority class: {mostCommon(sub_subset)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a440424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predefined routine tree:\n",
      "A5(+A4(---)A6(--)A1(--+))\n"
     ]
    }
   ],
   "source": [
    "tree = buildTree(dataset, m.attributes, maxdepth=2)\n",
    "print('predefined routine tree:')\n",
    "print(tree)\n",
    "#print(drawTree(tree)) #this kills the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f5cf58",
   "metadata": {},
   "source": [
    "### Assignment 5\n",
    "Build the full decision trees for all three Monk\n",
    "datasets using buildTree. Then, use the function check to measure the performance of the decision tree on both the training and\n",
    "test datasets.\n",
    "Compute the train and test set errors for the three Monk datasets\n",
    "for the full trees. Were your assumptions about the datasets correct?\n",
    "Explain the results you get for the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fba61e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1:\n",
      "A5(+A4(A1(A2(+--)A2(-++)A2(--+))A2(-+-)A3(A1(A2(+--)+A2(-++))A2(--A1(--+))))A6(A4(A3(A1(A2(+--)A2(-+-)-)-)A1(A2(+--)--)-)A3(A4(A1(+--)A1(-++)+)A1(-A2(-+-)A2(--+))))A1(A2(+--)A2(-+-)A2(+-+)))\n",
      "MONK-2:\n",
      "A5(A3(A6(-A1(-A2(--A4(--+))A2(-A4(-+-)-)))A4(A2(--A1(-+-))+A1(A2(-A6(-+)-)A2(A6(-+)++)+)))A3(A4(A2(-+-)A6(A1(--+)+)A1(A2(-++)++))A2(A4(++A1(+-A6(+-)))-A1(+A4(+--)-)))A3(A6(A1(-A4(-++)A4(--+))A2(+A1(+-A4(+--))A4(A1(-++)A1(+--)-)))A4(A2(A6(-+)A6(+-)A1(A6(-+)++))A1(A2(-+-)--)A1(A6(+-)--)))A2(A6(-A1(+-A4(++-)))A1(A4(-A3(-+)A3(A6(-+)A6(+-)))A3(+A6(+-))-)A3(A4(--+)-)))\n",
      "MONK-3:\n",
      "A2(A5(++A4(A1(--+)++)-)A5(+A1(+A4(+-+)+)A3(A4(+A1(--+)A1(+--))+)-)A4(A5(--+A1(--+))--))\n"
     ]
    }
   ],
   "source": [
    "test_datasets = [m.monk1test, m.monk2test, m.monk3test]\n",
    "performances = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    t = buildTree(dataset, m.attributes);\n",
    "    print(f'{dataset_names[i]}:')\n",
    "    print(t)\n",
    "    performance = {}\n",
    "    performance['dataset'] = dataset_names[i]\n",
    "    performance['Performance_train'] = check(t, dataset)\n",
    "    performance['Error_train'] = 1 - performance['Performance_train']\n",
    "    performance['Performance_test'] = check(t, test_datasets[i])\n",
    "    performance['Error_test'] = 1 - performance['Performance_test']\n",
    "    performances.append(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d959a899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| dataset   |   Performance_train |   Error_train |   Performance_test |   Error_test |\n",
      "|-----------|---------------------|---------------|--------------------|--------------|\n",
      "| MONK-1    |                   1 |             0 |           0.828704 |    0.171296  |\n",
      "| MONK-2    |                   1 |             0 |           0.69213  |    0.30787   |\n",
      "| MONK-3    |                   1 |             0 |           0.944444 |    0.0555556 |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(performances, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc4569",
   "metadata": {},
   "source": [
    "The assumption that dataset MONK-2 would be the most difficult one seems correct since it has both the most complet model (tree with more nodes) and the biggest error on the test dataset (even though the 3rd dataset is the one that contains noise!).\n",
    "\n",
    "All the errors are zero for the train dataset, which indicates that the model successfully learned from the train dataset. However, the error is not zero on the test sets, which means that the model does not generalize well, and is probably overfitting the training data. Besides, MONK-3 contains noise (misclassified samples) which will increase the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d493e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(data, fraction):\n",
    "    ldata = list(data)\n",
    "    random.shuffle(ldata)\n",
    "    breakPoint = int(len(ldata) * fraction)\n",
    "    return ldata[:breakPoint], ldata[breakPoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb7ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "monk1train, monk1val = partition(m.monk1, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5473724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daee83da",
   "metadata": {},
   "source": [
    "### Assignment 6\n",
    "Explain pruning from a bias variance trade-off perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf221b32",
   "metadata": {},
   "source": [
    "Ideally a model seeks low variance and low bias, but unfortunately there is a trade-off between the two (increasing bias reduces variance and vice versa).  \n",
    "Higher variance leads to overfitting of a model. The variance is determined by the depth of the tree. Deeper desision trees are more complex, have higher variance and higher chance of overfitting.  \n",
    "Prunning is a technique that allows the reduction of the model's complexity minimizing lost of predictive power. It is a effective approach to prevent overfitting. However, the bias is reduced, which can lead to decreased performance in more complex problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497ab68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
